(window.webpackJsonp=window.webpackJsonp||[]).push([[48],{479:function(t,a,s){"use strict";s.r(a);var n=s(20),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"mapreduce"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce"}},[t._v("#")]),t._v(" MapReduce")]),t._v(" "),s("h2",{attrs:{id:"一、mapreduce-简介"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#一、mapreduce-简介"}},[t._v("#")]),t._v(" 一、MapReduce 简介")]),t._v(" "),s("blockquote",[s("p",[t._v("Hadoop MapReduce 是一个分布式计算框架，用于编写批处理应用程序。编写好的程序可以提交到 Hadoop 集群上用于并行处理大规模的数据集。")])]),t._v(" "),s("p",[t._v("MapReduce 的设计思路是：")]),t._v(" "),s("ul",[s("li",[t._v("分而治之，并行计算")]),t._v(" "),s("li",[t._v("移动计算，而非移动数据")])]),t._v(" "),s("p",[t._v("MapReduce 作业通过将输入的数据集拆分为独立的块，这些块由 "),s("code",[t._v("map")]),t._v(" 以并行的方式处理，框架对 "),s("code",[t._v("map")]),t._v(" 的输出进行排序，然后输入到 "),s("code",[t._v("reduce")]),t._v(" 中。MapReduce 框架专门用于 "),s("code",[t._v("<key，value>")]),t._v(" 键值对处理，它将作业的输入视为一组 "),s("code",[t._v("<key，value>")]),t._v(" 对，并生成一组 "),s("code",[t._v("<key，value>")]),t._v(" 对作为输出。输出和输出的 "),s("code",[t._v("key")]),t._v(" 和 "),s("code",[t._v("value")]),t._v(" 都必须实现"),s("a",{attrs:{href:"http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Writable"),s("OutboundLink")],1),t._v(" 接口。")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("(input) <k1, v1> -> map -> <k2, v2> -> combine -> <k2, v2> -> reduce -> <k3, v3> (output)\n")])])]),s("h3",{attrs:{id:"特点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#特点"}},[t._v("#")]),t._v(" 特点")]),t._v(" "),s("ul",[s("li",[t._v("计算跟着数据走")]),t._v(" "),s("li",[t._v("良好的扩展性：计算能力随着节点数增加，近似线性递增")]),t._v(" "),s("li",[t._v("高容错")]),t._v(" "),s("li",[t._v("状态监控")]),t._v(" "),s("li",[t._v("适合海量数据的离线批处理")]),t._v(" "),s("li",[t._v("降低了分布式编程的门槛")])]),t._v(" "),s("h3",{attrs:{id:"应用场景"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#应用场景"}},[t._v("#")]),t._v(" 应用场景")]),t._v(" "),s("p",[t._v("适用场景：")]),t._v(" "),s("ul",[s("li",[t._v("数据统计，如：网站的 PV、UV 统计")]),t._v(" "),s("li",[t._v("搜索引擎构建索引")]),t._v(" "),s("li",[t._v("海量数据查询")])]),t._v(" "),s("p",[t._v("不适用场景：")]),t._v(" "),s("ul",[s("li",[t._v("OLAP\n"),s("ul",[s("li",[t._v("要求毫秒或秒级返回结果")])])]),t._v(" "),s("li",[t._v("流计算\n"),s("ul",[s("li",[t._v("流计算的输入数据集是动态的，而 MapReduce 是静态的")])])]),t._v(" "),s("li",[t._v("DAG 计算\n"),s("ul",[s("li",[t._v("多个作业存在依赖关系，后一个的输入是前一个的输出，构成有向无环图 DAG")]),t._v(" "),s("li",[t._v("每个 MapReduce 作业的输出结果都会落盘，造成大量磁盘 IO，导致性能非常低下")])])])]),t._v(" "),s("h2",{attrs:{id:"二、mapreduce-编程模型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#二、mapreduce-编程模型"}},[t._v("#")]),t._v(" 二、MapReduce 编程模型")]),t._v(" "),s("p",[t._v("MapReduce 编程模型：MapReduce 程序被分为 Map（映射）阶段和 Reduce（化简）阶段。")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/dunwu/images/dev/snap/20200601162305.png",alt:"img"}})]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("input")]),t._v(" : 读取文本文件；")]),t._v(" "),s("li",[s("strong",[t._v("splitting")]),t._v(" : 将文件按照行进行拆分，此时得到的 "),s("code",[t._v("K1")]),t._v(" 行数，"),s("code",[t._v("V1")]),t._v(" 表示对应行的文本内容；")]),t._v(" "),s("li",[s("strong",[t._v("mapping")]),t._v(" : 并行将每一行按照空格进行拆分，拆分得到的 "),s("code",[t._v("List(K2,V2)")]),t._v("，其中 "),s("code",[t._v("K2")]),t._v(" 代表每一个单词，由于是做词频统计，所以 "),s("code",[t._v("V2")]),t._v(" 的值为 1，代表出现 1 次；")]),t._v(" "),s("li",[s("strong",[t._v("shuffling")]),t._v("：由于 "),s("code",[t._v("Mapping")]),t._v(" 操作可能是在不同的机器上并行处理的，所以需要通过 "),s("code",[t._v("shuffling")]),t._v(" 将相同 "),s("code",[t._v("key")]),t._v(" 值的数据分发到同一个节点上去合并，这样才能统计出最终的结果，此时得到 "),s("code",[t._v("K2")]),t._v(" 为每一个单词，"),s("code",[t._v("List(V2)")]),t._v(" 为可迭代集合，"),s("code",[t._v("V2")]),t._v(" 就是 Mapping 中的 V2；")]),t._v(" "),s("li",[s("strong",[t._v("Reducing")]),t._v(" : 这里的案例是统计单词出现的总次数，所以 "),s("code",[t._v("Reducing")]),t._v(" 对 "),s("code",[t._v("List(V2)")]),t._v(" 进行归约求和操作，最终输出。")])]),t._v(" "),s("p",[t._v("MapReduce 编程模型中 "),s("code",[t._v("splitting")]),t._v(" 和 "),s("code",[t._v("shuffing")]),t._v(" 操作都是由框架实现的，需要我们自己编程实现的只有 "),s("code",[t._v("mapping")]),t._v(" 和 "),s("code",[t._v("reducing")]),t._v("，这也就是 MapReduce 这个称呼的来源。")]),t._v(" "),s("h2",{attrs:{id:"三、combiner-partitioner"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#三、combiner-partitioner"}},[t._v("#")]),t._v(" 三、combiner & partitioner")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/dunwu/images/dev/snap/20200601163846.png",alt:"img"}})]),t._v(" "),s("h3",{attrs:{id:"_3-1-inputformat-recordreaders"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-inputformat-recordreaders"}},[t._v("#")]),t._v(" 3.1 InputFormat & RecordReaders")]),t._v(" "),s("p",[s("code",[t._v("InputFormat")]),t._v(" 将输出文件拆分为多个 "),s("code",[t._v("InputSplit")]),t._v("，并由 "),s("code",[t._v("RecordReaders")]),t._v(" 将 "),s("code",[t._v("InputSplit")]),t._v(" 转换为标准的<key，value>键值对，作为 map 的输出。这一步的意义在于只有先进行逻辑拆分并转为标准的键值对格式后，才能为多个 "),s("code",[t._v("map")]),t._v(" 提供输入，以便进行并行处理。")]),t._v(" "),s("h3",{attrs:{id:"_3-2-combiner"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-combiner"}},[t._v("#")]),t._v(" 3.2 Combiner")]),t._v(" "),s("p",[s("code",[t._v("combiner")]),t._v(" 是 "),s("code",[t._v("map")]),t._v(" 运算后的可选操作，它实际上是一个本地化的 "),s("code",[t._v("reduce")]),t._v(" 操作，它主要是在 "),s("code",[t._v("map")]),t._v(" 计算出中间文件后做一个简单的合并重复 "),s("code",[t._v("key")]),t._v(" 值的操作。这里以词频统计为例：")]),t._v(" "),s("p",[s("code",[t._v("map")]),t._v(" 在遇到一个 hadoop 的单词时就会记录为 1，但是这篇文章里 hadoop 可能会出现 n 多次，那么 "),s("code",[t._v("map")]),t._v(" 输出文件冗余就会很多，因此在 "),s("code",[t._v("reduce")]),t._v(" 计算前对相同的 key 做一个合并操作，那么需要传输的数据量就会减少，传输效率就可以得到提升。")]),t._v(" "),s("p",[t._v("但并非所有场景都适合使用 "),s("code",[t._v("combiner")]),t._v("，使用它的原则是 "),s("code",[t._v("combiner")]),t._v(" 的输出不会影响到 "),s("code",[t._v("reduce")]),t._v(" 计算的最终输入，例如：求总数，最大值，最小值时都可以使用 "),s("code",[t._v("combiner")]),t._v("，但是做平均值计算则不能使用 "),s("code",[t._v("combiner")]),t._v("。")]),t._v(" "),s("p",[t._v("不使用 combiner 的情况：")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/dunwu/images/dev/snap/20200601164709.png",alt:"img"}})]),t._v(" "),s("p",[t._v("使用 combiner 的情况：")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/dunwu/images/dev/snap/20200601164804.png",alt:"img"}})]),t._v(" "),s("p",[t._v("可以看到使用 combiner 的时候，需要传输到 reducer 中的数据由 12keys，降低到 10keys。降低的幅度取决于你 keys 的重复率，下文词频统计案例会演示用 combiner 降低数百倍的传输量。")]),t._v(" "),s("h2",{attrs:{id:"四、mapreduce词频统计案例"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#四、mapreduce词频统计案例"}},[t._v("#")]),t._v(" 四、MapReduce词频统计案例")]),t._v(" "),s("h3",{attrs:{id:"_4-1-项目简介"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-项目简介"}},[t._v("#")]),t._v(" 4.1 项目简介")]),t._v(" "),s("p",[t._v("这里给出一个经典的词频统计的案例：统计如下样本数据中每个单词出现的次数。")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("Spark\tHBase\nHive\tFlink\tStorm\tHadoop\tHBase\tSpark\nFlink\nHBase\tStorm\nHBase\tHadoop\tHive\tFlink\nHBase\tFlink\tHive\tStorm\nHive\tFlink\tHadoop\nHBase\tHive\nHadoop\tSpark\tHBase\tStorm\nHBase\tHadoop\tHive\tFlink\nHBase\tFlink\tHive\tStorm\nHive\tFlink\tHadoop\nHBase\tHive\n")])])]),s("p",[t._v("为方便大家开发，我在项目源码中放置了一个工具类 "),s("code",[t._v("WordCountDataUtils")]),t._v("，用于模拟产生词频统计的样本，生成的文件支持输出到本地或者直接写到 HDFS 上。")]),t._v(" "),s("blockquote",[s("p",[t._v("项目完整源码下载地址："),s("a",{attrs:{href:"https://github.com/heibaiying/BigData-Notes/tree/master/code/Hadoop/hadoop-word-count",target:"_blank",rel:"noopener noreferrer"}},[t._v("hadoop-word-count"),s("OutboundLink")],1)])]),t._v(" "),s("h3",{attrs:{id:"_4-2-项目依赖"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-项目依赖"}},[t._v("#")]),t._v(" 4.2 项目依赖")]),t._v(" "),s("p",[t._v("想要进行 MapReduce 编程，需要导入 "),s("code",[t._v("hadoop-client")]),t._v(" 依赖：")]),t._v(" "),s("div",{staticClass:"language-xml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-xml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("dependency")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("groupId")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("org.apache.hadoop"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("groupId")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("artifactId")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("hadoop-client"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("artifactId")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("version")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("${hadoop.version}"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("version")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("dependency")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])])]),s("h3",{attrs:{id:"_4-3-wordcountmapper"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-wordcountmapper"}},[t._v("#")]),t._v(" 4.3 WordCountMapper")]),t._v(" "),s("p",[t._v("将每行数据按照指定分隔符进行拆分。这里需要注意在 MapReduce 中必须使用 Hadoop 定义的类型，因为 Hadoop 预定义的类型都是可序列化，可比较的，所有类型均实现了 "),s("code",[t._v("WritableComparable")]),t._v(" 接口。")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WordCountMapper")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("extends")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Mapper")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("LongWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("protected")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("LongWritable")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),t._v(" value"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Context")]),t._v(" context"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                                                                      "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("InterruptedException")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" words "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" value"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("toString")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("split")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\\t"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),t._v(" word "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" words"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            context"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("write")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("word"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[s("code",[t._v("WordCountMapper")]),t._v(" 对应下图的 Mapping 操作：")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://camo.githubusercontent.com/fcf3ac016579c1fbb050d97309660aaa3b9550cc/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d636f64652d6d617070696e672e706e67",target:"_blank",rel:"noopener noreferrer"}},[s("img",{attrs:{src:"https://camo.githubusercontent.com/fcf3ac016579c1fbb050d97309660aaa3b9550cc/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d636f64652d6d617070696e672e706e67",alt:"img"}}),s("OutboundLink")],1)]),t._v(" "),s("p",[s("code",[t._v("WordCountMapper")]),t._v(" 继承自 "),s("code",[t._v("Mappe")]),t._v(" 类，这是一个泛型类，定义如下：")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WordCountMapper")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("extends")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Mapper")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("LongWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Mapper")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("KEYIN"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VALUEIN"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" KEYOUT"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VALUEOUT"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n   "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("ul",[s("li",[s("strong",[t._v("KEYIN")]),t._v(" : "),s("code",[t._v("mapping")]),t._v(" 输入 key 的类型，即每行的偏移量 (每行第一个字符在整个文本中的位置)，"),s("code",[t._v("Long")]),t._v(" 类型，对应 Hadoop 中的 "),s("code",[t._v("LongWritable")]),t._v(" 类型；")]),t._v(" "),s("li",[s("strong",[t._v("VALUEIN")]),t._v(" : "),s("code",[t._v("mapping")]),t._v(" 输入 value 的类型，即每行数据；"),s("code",[t._v("String")]),t._v(" 类型，对应 Hadoop 中 "),s("code",[t._v("Text")]),t._v(" 类型；")]),t._v(" "),s("li",[s("strong",[t._v("KEYOUT")]),t._v(" ："),s("code",[t._v("mapping")]),t._v(" 输出的 key 的类型，即每个单词；"),s("code",[t._v("String")]),t._v(" 类型，对应 Hadoop 中 "),s("code",[t._v("Text")]),t._v(" 类型；")]),t._v(" "),s("li",[s("strong",[t._v("VALUEOUT")]),t._v("："),s("code",[t._v("mapping")]),t._v(" 输出 value 的类型，即每个单词出现的次数；这里用 "),s("code",[t._v("int")]),t._v(" 类型，对应 "),s("code",[t._v("IntWritable")]),t._v(" 类型。")])]),t._v(" "),s("h3",{attrs:{id:"_4-4-wordcountreducer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-4-wordcountreducer"}},[t._v("#")]),t._v(" 4.4 WordCountReducer")]),t._v(" "),s("p",[t._v("在 Reduce 中进行单词出现次数的统计：")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WordCountReducer")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("extends")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Reducer")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("protected")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("reduce")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Iterable")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Context")]),t._v(" context"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                                                                                  "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("InterruptedException")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" count "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),t._v(" value "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            count "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" value"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("get")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        context"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("write")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("count"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("如下图，"),s("code",[t._v("shuffling")]),t._v(" 的输出是 reduce 的输入。这里的 key 是每个单词，values 是一个可迭代的数据类型，类似 "),s("code",[t._v("(1,1,1,...)")]),t._v("。")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://camo.githubusercontent.com/bf6ad9c970812b1db6f6f86d12d783db75eaa9fb/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d636f64652d726564756365722e706e67",target:"_blank",rel:"noopener noreferrer"}},[s("img",{attrs:{src:"https://camo.githubusercontent.com/bf6ad9c970812b1db6f6f86d12d783db75eaa9fb/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d636f64652d726564756365722e706e67",alt:"img"}}),s("OutboundLink")],1)]),t._v(" "),s("h3",{attrs:{id:"_4-4-wordcountapp"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-4-wordcountapp"}},[t._v("#")]),t._v(" 4.4 WordCountApp")]),t._v(" "),s("p",[t._v("组装 MapReduce 作业，并提交到服务器运行，代码如下：")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/**\n * 组装作业 并提交到集群运行\n */")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WordCountApp")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\n\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 这里为了直观显示参数 使用了硬编码，实际开发中可以通过外部传参")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("private")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("static")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("final")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),t._v(" HDFS_URL "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hdfs://192.168.0.107:8020"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("private")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("static")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("final")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),t._v(" HADOOP_USER_NAME "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"root"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("static")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" args"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Exception")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//  文件输入路径和输出路径由外部传参指定")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("length "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("System")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("println")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Input and output paths are necessary!"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 需要指明 hadoop 用户名，否则在 HDFS 上创建目录时可能会抛出权限不足的异常")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("System")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setProperty")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"HADOOP_USER_NAME"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" HADOOP_USER_NAME"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Configuration")]),t._v(" configuration "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Configuration")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 指明 HDFS 的地址")]),t._v("\n        configuration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("set")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"fs.defaultFS"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" HDFS_URL"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 创建一个 Job")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Job")]),t._v(" job "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Job")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getInstance")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("configuration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 设置运行的主类")]),t._v("\n        job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setJarByClass")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WordCountApp")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 设置 Mapper 和 Reducer")]),t._v("\n        job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setMapperClass")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WordCountMapper")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setReducerClass")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WordCountReducer")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 设置 Mapper 输出 key 和 value 的类型")]),t._v("\n        job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setMapOutputKeyClass")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setMapOutputValueClass")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 设置 Reducer 输出 key 和 value 的类型")]),t._v("\n        job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setOutputKeyClass")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setOutputValueClass")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 如果输出目录已经存在，则必须先删除，否则重复运行程序时会抛出异常")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileSystem")]),t._v(" fileSystem "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileSystem")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("get")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("URI")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("HDFS_URL"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" configuration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" HADOOP_USER_NAME"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),t._v(" outputPath "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fileSystem"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("exists")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("outputPath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            fileSystem"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("delete")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("outputPath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 设置作业输入文件和输出文件的路径")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileInputFormat")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setInputPaths")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileOutputFormat")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setOutputPath")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" outputPath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 将作业提交到群集并等待它完成，参数设置为 true 代表打印显示对应的进度")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("boolean")]),t._v(" result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("waitForCompletion")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 关闭之前创建的 fileSystem")]),t._v("\n        fileSystem"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("close")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 根据作业结果,终止当前运行的 Java 虚拟机,退出程序")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("System")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("exit")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("?")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("需要注意的是：如果不设置 "),s("code",[t._v("Mapper")]),t._v(" 操作的输出类型，则程序默认它和 "),s("code",[t._v("Reducer")]),t._v(" 操作输出的类型相同。")]),t._v(" "),s("h3",{attrs:{id:"_4-5-提交到服务器运行"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-5-提交到服务器运行"}},[t._v("#")]),t._v(" 4.5 提交到服务器运行")]),t._v(" "),s("p",[t._v("在实际开发中，可以在本机配置 hadoop 开发环境，直接在 IDE 中启动进行测试。这里主要介绍一下打包提交到服务器运行。由于本项目没有使用除 Hadoop 外的第三方依赖，直接打包即可：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("# mvn clean package\n")])])]),s("p",[t._v("使用以下命令提交作业：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("hadoop jar /usr/appjar/hadoop-word-count-1.0.jar \\\ncom.heibaiying.WordCountApp \\\n/wordcount/input.txt /wordcount/output/WordCountApp\n")])])]),s("p",[t._v("作业完成后查看 HDFS 上生成目录：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("# 查看目录\nhadoop fs -ls /wordcount/output/WordCountApp\n\n# 查看统计结果\nhadoop fs -cat /wordcount/output/WordCountApp/part-r-00000\n")])])]),s("p",[s("a",{attrs:{href:"https://camo.githubusercontent.com/cecb00eef3b951794fbf92b8308d8b6601faf5a0/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d776f7264636f756e746170702e706e67",target:"_blank",rel:"noopener noreferrer"}},[s("img",{attrs:{src:"https://camo.githubusercontent.com/cecb00eef3b951794fbf92b8308d8b6601faf5a0/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d776f7264636f756e746170702e706e67",alt:"img"}}),s("OutboundLink")],1)]),t._v(" "),s("h2",{attrs:{id:"五、词频统计案例进阶之combiner"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#五、词频统计案例进阶之combiner"}},[t._v("#")]),t._v(" 五、词频统计案例进阶之Combiner")]),t._v(" "),s("h3",{attrs:{id:"_5-1-代码实现"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-代码实现"}},[t._v("#")]),t._v(" 5.1 代码实现")]),t._v(" "),s("p",[t._v("想要使用 "),s("code",[t._v("combiner")]),t._v(" 功能只要在组装作业时，添加下面一行代码即可：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("// 设置 Combiner\njob.setCombinerClass(WordCountReducer.class);\n")])])]),s("h3",{attrs:{id:"_5-2-执行结果"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-执行结果"}},[t._v("#")]),t._v(" 5.2 执行结果")]),t._v(" "),s("p",[t._v("加入 "),s("code",[t._v("combiner")]),t._v(" 后统计结果是不会有变化的，但是可以从打印的日志看出 "),s("code",[t._v("combiner")]),t._v(" 的效果：")]),t._v(" "),s("p",[t._v("没有加入 "),s("code",[t._v("combiner")]),t._v(" 的打印日志：")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://camo.githubusercontent.com/e4849556db34d3a02b82b546af7296154920dfff/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d6e6f2d636f6d62696e65722e706e67",target:"_blank",rel:"noopener noreferrer"}},[s("img",{attrs:{src:"https://camo.githubusercontent.com/e4849556db34d3a02b82b546af7296154920dfff/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d6e6f2d636f6d62696e65722e706e67",alt:"img"}}),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("加入 "),s("code",[t._v("combiner")]),t._v(" 后的打印日志如下：")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://camo.githubusercontent.com/17f20f481bc4bd01252bc3ccb3b2aceb7d0eca63/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d636f6d62696e65722e706e67",target:"_blank",rel:"noopener noreferrer"}},[s("img",{attrs:{src:"https://camo.githubusercontent.com/17f20f481bc4bd01252bc3ccb3b2aceb7d0eca63/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d636f6d62696e65722e706e67",alt:"img"}}),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("这里我们只有一个输入文件并且小于 128M，所以只有一个 Map 进行处理。可以看到经过 combiner 后，records 由 "),s("code",[t._v("3519")]),t._v(" 降低为 "),s("code",[t._v("6")]),t._v("(样本中单词种类就只有 6 种)，在这个用例中 combiner 就能极大地降低需要传输的数据量。")]),t._v(" "),s("h2",{attrs:{id:"六、词频统计案例进阶之partitioner"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#六、词频统计案例进阶之partitioner"}},[t._v("#")]),t._v(" 六、词频统计案例进阶之Partitioner")]),t._v(" "),s("h3",{attrs:{id:"_6-1-默认的partitioner"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_6-1-默认的partitioner"}},[t._v("#")]),t._v(" 6.1 默认的Partitioner")]),t._v(" "),s("p",[t._v("这里假设有个需求：将不同单词的统计结果输出到不同文件。这种需求实际上比较常见，比如统计产品的销量时，需要将结果按照产品种类进行拆分。要实现这个功能，就需要用到自定义 "),s("code",[t._v("Partitioner")]),t._v("。")]),t._v(" "),s("p",[t._v("这里先介绍下 MapReduce 默认的分类规则：在构建 job 时候，如果不指定，默认的使用的是 "),s("code",[t._v("HashPartitioner")]),t._v("：对 key 值进行哈希散列并对 "),s("code",[t._v("numReduceTasks")]),t._v(" 取余。其实现如下：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("public class HashPartitioner<K, V> extends Partitioner<K, V> {\n\n  public int getPartition(K key, V value,\n                          int numReduceTasks) {\n    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n  }\n\n}\n")])])]),s("h3",{attrs:{id:"_6-2-自定义partitioner"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_6-2-自定义partitioner"}},[t._v("#")]),t._v(" 6.2 自定义Partitioner")]),t._v(" "),s("p",[t._v("这里我们继承 "),s("code",[t._v("Partitioner")]),t._v(" 自定义分类规则，这里按照单词进行分类：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("public class CustomPartitioner extends Partitioner<Text, IntWritable> {\n\n    public int getPartition(Text text, IntWritable intWritable, int numPartitions) {\n        return WordCountDataUtils.WORD_LIST.indexOf(text.toString());\n    }\n}\n")])])]),s("p",[t._v("在构建 "),s("code",[t._v("job")]),t._v(" 时候指定使用我们自己的分类规则，并设置 "),s("code",[t._v("reduce")]),t._v(" 的个数：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("// 设置自定义分区规则\njob.setPartitionerClass(CustomPartitioner.class);\n// 设置 reduce 个数\njob.setNumReduceTasks(WordCountDataUtils.WORD_LIST.size());\n")])])]),s("h3",{attrs:{id:"_6-3-执行结果"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_6-3-执行结果"}},[t._v("#")]),t._v(" 6.3 执行结果")]),t._v(" "),s("p",[t._v("执行结果如下，分别生成 6 个文件，每个文件中为对应单词的统计结果：")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://camo.githubusercontent.com/202b1eb7065e18a513db5b2a50b22ab62a7d6692/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d776f7264636f756e74636f6d62696e6572706172746974696f6e2e706e67",target:"_blank",rel:"noopener noreferrer"}},[s("img",{attrs:{src:"https://camo.githubusercontent.com/202b1eb7065e18a513db5b2a50b22ab62a7d6692/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d776f7264636f756e74636f6d62696e6572706172746974696f6e2e706e67",alt:"img"}}),s("OutboundLink")],1)]),t._v(" "),s("h2",{attrs:{id:"参考资料"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"https://github.com/heibaiying/BigData-Notes/blob/master/notes/Hadoop-MapReduce.md",target:"_blank",rel:"noopener noreferrer"}},[t._v("分布式计算框架——MapReduce"),s("OutboundLink")],1)])])])}),[],!1,null,null,null);a.default=e.exports}}]);