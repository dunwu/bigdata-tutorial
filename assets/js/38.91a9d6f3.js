(window.webpackJsonp=window.webpackJsonp||[]).push([[38],{470:function(t,s,a){"use strict";a.r(s);var e=a(20),n=Object(e.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"hive-分区表和分桶表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive-分区表和分桶表"}},[t._v("#")]),t._v(" Hive 分区表和分桶表")]),t._v(" "),a("h2",{attrs:{id:"一、分区表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#一、分区表"}},[t._v("#")]),t._v(" 一、分区表")]),t._v(" "),a("h3",{attrs:{id:"概念"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#概念"}},[t._v("#")]),t._v(" 概念")]),t._v(" "),a("p",[t._v("Hive 中的表对应为 HDFS 上的指定目录，在查询数据时候，默认会对全表进行扫描，这样时间和性能的消耗都非常大。")]),t._v(" "),a("p",[a("strong",[t._v("分区为 HDFS 上表目录的子目录")]),t._v("，数据按照分区存储在子目录中。如果查询的 "),a("code",[t._v("where")]),t._v(" 子句中包含分区条件，则直接从该分区去查找，而不是扫描整个表目录，合理的分区设计可以极大提高查询速度和性能。")]),t._v(" "),a("blockquote",[a("p",[t._v("这里说明一下分区表并非 Hive 独有的概念，实际上这个概念非常常见。比如在我们常用的 Oracle 数据库中，当表中的数据量不断增大，查询数据的速度就会下降，这时也可以对表进行分区。表进行分区后，逻辑上表仍然是一张完整的表，只是将表中的数据存放到多个表空间（物理文件上），这样查询数据时，就不必要每次都扫描整张表，从而提升查询性能。")])]),t._v(" "),a("h3",{attrs:{id:"使用场景"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#使用场景"}},[t._v("#")]),t._v(" 使用场景")]),t._v(" "),a("p",[t._v("通常，在管理大规模数据集的时候都需要进行分区，比如将日志文件按天进行分区，从而保证数据细粒度的划分，使得查询性能得到提升。")]),t._v(" "),a("h3",{attrs:{id:"创建分区表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#创建分区表"}},[t._v("#")]),t._v(" 创建分区表")]),t._v(" "),a("p",[t._v("在 Hive 中可以使用 "),a("code",[t._v("PARTITIONED BY")]),t._v(" 子句创建分区表。表可以包含一个或多个分区列，程序会为分区列中的每个不同值组合创建单独的数据目录。下面的我们创建一张雇员表作为测试：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CREATE")]),t._v(" EXTERNAL "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" emp_partition"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    empno "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    ename STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    job STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    mgr "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    hiredate "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TIMESTAMP")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    sal "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DECIMAL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    comm "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DECIMAL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    PARTITIONED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("deptno "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 按照部门编号进行分区")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ROW")]),t._v(" FORMAT DELIMITED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FIELDS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TERMINATED")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\\t"')]),t._v("\n    LOCATION "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/hive/emp_partition'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h3",{attrs:{id:"加载数据到分区表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#加载数据到分区表"}},[t._v("#")]),t._v(" 加载数据到分区表")]),t._v(" "),a("p",[t._v("加载数据到分区表时候必须要指定数据所处的分区：")]),t._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 加载部门编号为20的数据到表中")]),t._v("\nLOAD DATA LOCAL INPATH "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/usr/file/emp20.txt"')]),t._v(" OVERWRITE INTO TABLE emp_partition PARTITION "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("deptno"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 加载部门编号为30的数据到表中")]),t._v("\nLOAD DATA LOCAL INPATH "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/usr/file/emp30.txt"')]),t._v(" OVERWRITE INTO TABLE emp_partition PARTITION "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("deptno"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h3",{attrs:{id:"查看分区目录"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#查看分区目录"}},[t._v("#")]),t._v(" 查看分区目录")]),t._v(" "),a("p",[t._v("这时候我们直接查看表目录，可以看到表目录下存在两个子目录，分别是 "),a("code",[t._v("deptno=20")]),t._v(" 和 "),a("code",[t._v("deptno=30")]),t._v(",这就是分区目录，分区目录下才是我们加载的数据文件。")]),t._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# hadoop fs -ls  hdfs://hadoop001:8020/hive/emp_partition/")]),t._v("\n")])])]),a("p",[t._v("这时候当你的查询语句的 "),a("code",[t._v("where")]),t._v(" 包含 "),a("code",[t._v("deptno=20")]),t._v("，则就去对应的分区目录下进行查找，而不用扫描全表。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-hadoop-partitation.png",alt:"img"}})]),t._v(" "),a("h2",{attrs:{id:"二、分桶表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#二、分桶表"}},[t._v("#")]),t._v(" 二、分桶表")]),t._v(" "),a("h3",{attrs:{id:"简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#简介"}},[t._v("#")]),t._v(" 简介")]),t._v(" "),a("p",[t._v("分区提供了一个隔离数据和优化查询的可行方案，但是并非所有的数据集都可以形成合理的分区，分区的数量也不是越多越好，过多的分区条件可能会导致很多分区上没有数据。同时 Hive 会限制动态分区可以创建的最大分区数，用来避免过多分区文件对文件系统产生负担。鉴于以上原因，Hive 还提供了一种更加细粒度的数据拆分方案：分桶表 (bucket Table)。")]),t._v(" "),a("p",[t._v("分桶表会将指定列的值进行哈希散列，并对 bucket（桶数量）取余，然后存储到对应的 bucket（桶）中。")]),t._v(" "),a("h3",{attrs:{id:"理解分桶表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#理解分桶表"}},[t._v("#")]),t._v(" 理解分桶表")]),t._v(" "),a("p",[t._v("单从概念上理解分桶表可能会比较晦涩，其实和分区一样，分桶这个概念同样不是 Hive 独有的，对于 Java 开发人员而言，这可能是一个每天都会用到的概念，因为 Hive 中的分桶概念和 Java 数据结构中的 HashMap 的分桶概念是一致的。")]),t._v(" "),a("p",[t._v("当调用 HashMap 的 put() 方法存储数据时，程序会先对 key 值调用 hashCode() 方法计算出 hashcode，然后对数组长度取模计算出 index，最后将数据存储在数组 index 位置的链表上，链表达到一定阈值后会转换为红黑树 (JDK1.8+)。下图为 HashMap 的数据结构图：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://raw.githubusercontent.com/dunwu/images/dev/snap/20200224194352.png",alt:"img"}})]),t._v(" "),a("p",[t._v("图片引用自："),a("a",{attrs:{href:"http://www.itcuties.com/java/hashmap-hashtable/",target:"_blank",rel:"noopener noreferrer"}},[t._v("HashMap vs. Hashtable"),a("OutboundLink")],1)]),t._v(" "),a("h3",{attrs:{id:"创建分桶表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#创建分桶表"}},[t._v("#")]),t._v(" 创建分桶表")]),t._v(" "),a("p",[t._v("在 Hive 中，我们可以通过 "),a("code",[t._v("CLUSTERED BY")]),t._v(" 指定分桶列，并通过 "),a("code",[t._v("SORTED BY")]),t._v(" 指定桶中数据的排序参考列。下面为分桶表建表语句示例：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CREATE")]),t._v(" EXTERNAL "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" emp_bucket"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    empno "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    ename STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    job STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    mgr "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    hiredate "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TIMESTAMP")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    sal "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DECIMAL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    comm "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DECIMAL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    deptno "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CLUSTERED")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("empno"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" SORTED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("empno "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ASC")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INTO")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v(" BUCKETS  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("--按照员工编号散列到四个 bucket 中")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ROW")]),t._v(" FORMAT DELIMITED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FIELDS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TERMINATED")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\\t"')]),t._v("\n    LOCATION "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/hive/emp_bucket'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h3",{attrs:{id:"加载数据到分桶表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#加载数据到分桶表"}},[t._v("#")]),t._v(" 加载数据到分桶表")]),t._v(" "),a("p",[t._v("这里直接使用 "),a("code",[t._v("Load")]),t._v(" 语句向分桶表加载数据，数据时可以加载成功的，但是数据并不会分桶。")]),t._v(" "),a("p",[t._v("这是由于分桶的实质是对指定字段做了 hash 散列然后存放到对应文件中，这意味着向分桶表中插入数据是必然要通过 MapReduce，且 Reducer 的数量必须等于分桶的数量。由于以上原因，分桶表的数据通常只能使用 CTAS(CREATE TABLE AS SELECT) 方式插入，因为 CTAS 操作会触发 MapReduce。加载数据步骤如下：")]),t._v(" "),a("h4",{attrs:{id:"设置强制分桶"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#设置强制分桶"}},[t._v("#")]),t._v(" 设置强制分桶")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("enforce"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bucketing "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("--Hive 2.x 不需要这一步")]),t._v("\n")])])]),a("p",[t._v("在 Hive 0.x and 1.x 版本，必须使用设置 "),a("code",[t._v("hive.enforce.bucketing = true")]),t._v("，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by column 来进行分桶。")]),t._v(" "),a("h4",{attrs:{id:"ctas-导入数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ctas-导入数据"}},[t._v("#")]),t._v(" CTAS 导入数据")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INSERT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INTO")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" emp_bucket "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("--这里的 emp 表就是一张普通的雇员表")]),t._v("\n")])])]),a("p",[t._v("可以从执行日志看到 CTAS 触发 MapReduce 操作，且 Reducer 数量和建表时候指定 bucket 数量一致：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-hadoop-mapreducer.png",alt:"img"}})]),t._v(" "),a("h3",{attrs:{id:"查看分桶文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#查看分桶文件"}},[t._v("#")]),t._v(" 查看分桶文件")]),t._v(" "),a("p",[t._v("bucket(桶) 本质上就是表目录下的具体文件：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-hadoop-bucket.png",alt:"img"}})]),t._v(" "),a("h2",{attrs:{id:"三、分区表和分桶表结合使用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#三、分区表和分桶表结合使用"}},[t._v("#")]),t._v(" 三、分区表和分桶表结合使用")]),t._v(" "),a("p",[t._v("分区表和分桶表的本质都是将数据按照不同粒度进行拆分，从而使得在查询时候不必扫描全表，只需要扫描对应的分区或分桶，从而提升查询效率。两者可以结合起来使用，从而保证表数据在不同粒度上都能得到合理的拆分。下面是 Hive 官方给出的示例：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CREATE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" page_view_bucketed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n\tviewTime "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    userid "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BIGINT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    page_url STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    referrer_url STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    ip STRING "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n PARTITIONED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dt STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CLUSTERED")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("userid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" SORTED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("viewTime"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INTO")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),t._v(" BUCKETS\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ROW")]),t._v(" FORMAT DELIMITED\n   "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FIELDS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TERMINATED")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\001'")]),t._v("\n   COLLECTION ITEMS "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TERMINATED")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\002'")]),t._v("\n   MAP "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("KEYS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TERMINATED")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\003'")]),t._v("\n STORED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" SEQUENCEFILE"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("此时导入数据时需要指定分区：")]),t._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[t._v("INSERT OVERWRITE page_view_bucketed\nPARTITION "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dt"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2009-02-25'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nSELECT * FROM page_view WHERE "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("dt")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2009-02-25'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h2",{attrs:{id:"参考资料"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL+BucketedTables",target:"_blank",rel:"noopener noreferrer"}},[t._v("LanguageManual DDL BucketedTables"),a("OutboundLink")],1)])])])}),[],!1,null,null,null);s.default=n.exports}}]);