# Kafka 集群

> Kafka 是一个分布式的、可水平扩展的、基于发布/订阅模式的、支持容错的消息系统。

<!-- TOC depthFrom:2 depthTo:3 -->

- [一、集群成员](#一集群成员)
- [二、控制器](#二控制器)
- [三、复制](#三复制)
- [四、处理请求](#四处理请求)
  - [元数据请求](#元数据请求)
  - [生产请求](#生产请求)
  - [消费请求](#消费请求)
  - [其他请求](#其他请求)
- [五、物理存储](#五物理存储)
  - [分区分配](#分区分配)
  - [文件管理](#文件管理)
  - [文件格式](#文件格式)
  - [索引](#索引)
  - [清理](#清理)
  - [删除事件](#删除事件)
- [参考资料](#参考资料)

<!-- /TOC -->

## 一、集群成员

**Kafka 使用 Zookeeper 来维护集群成员的信息**。每个 broker 都有一个唯一标识符，这个标识符可以在配置文件里指定，也可以自动生成。在 broker 启动的时候，它通过创建**临时节点**把自己的 ID 注册到 Zookeeper。Kafka 组件订阅 Zookeeper 的 `/broker/ids` 路径，当有 broker 加入集群或退出集群时，这些组件就可以获得通知。

> ZooKeeper 两个重要特性：
>
> - 客户端会话结束时，ZooKeeper 就会删除临时节点。
> - 客户端注册监听它关心的节点，当节点状态发生变化（数据变化、子节点增减变化）时，ZooKeeper 服务会通知客户端。
>
> 详细内容可以参考：[ZooKeeper 应用指南](https://github.com/dunwu/bigdata-tutorial/blob/master/docs/zookeeper/zookeeper-quickstart.md)

## 二、控制器

控制器其实就是一个 broker。

（1）集群里第一个启动的 broker 通过在 Zookeeper 里创建一个临时节点 `/controller` 让自己成为控制器。

（2）其他 broker 在控制器节点上创建 Zookeeper watch 对象。

（3）如果控制器被关闭或者与 Zookeeper 断开连接，Zookeeper 临时节点就会消失。集群中的其他 broker 通过 watch 对象得到状态变化的通知，它们会尝试让自己成为新的控制器。

（4）第一个在 Zookeeper 里创建一个临时节点 `/controller` 的 broker 成为新控制器。其他 broker 在新控制器节点上创建 Zookeeper watch 对象。

（5）每个新选出的控制器通过 Zookeeper 的条件递增操作获得一个全新的、数值更大的 controller epoch。其他节点会忽略旧的 epoch 的消息。

（6）当一个 broker 离开集群，并且这个 broker 是某些分区的 Leader。此时，控制器会遍历这些分区，并用轮询方式确定谁应该成为新首领，随后，新首领开始处理生产者和消费者的请求，而跟随者开始从首领那里复制消息。

简而言之，**Kafka 使用 Zookeeper 的临时节点来选举控制器，并在节点加入集群或退出集群时通知控制器。控制器负责在节点加入或离开集群时进行分区首领选举。控制器使用 epoch 来避免“脑裂”，“脑裂”是指两个节点同时被认为自己是当前的控制器**。

## 三、复制

复制功能是 Kafka 架构的核心。复制之所以关键，是因为它可以在个别节点失效时仍能保证 Kafka 的可用性和持久性。

Kafka 使用主题来组织数据，每个主题被分为若干个分区，每个分区有多个副本。每个 broker 可以保存成百上千个属于不同主题和分区的副本。

副本有两种类型：

- Leader 副本（主）：每个分区都有一个 Leader 副本。**为了保证一致性，所有生产者请求和消费者请求都会经过这个副本**。
- Follower 副本（从）：Leader 副本以外的副本都是 Follower 副本。**跟随者副本不处理来自客户端的请求，它们唯一的任务就是从首领那里复制消息，保持与首领一致的状态**。如果 Leader 宕机，其中一个 Follower 会被选举为新的 Leader。

为了与首领保持同步，跟随者向首领发送获取数据的请求，这种请求与消费者为了读取消息而发送的请求是一样的。请求消息里包含了跟随者想要获取消息的偏移量，而这些偏移量总是有序的。通过查看每个跟随者请求的最新偏移量，首领就会知道每个跟随者复制的进度。如果跟随者在 10s 内没有请求任何消息，或者虽然在请求消息，但是 10s 内没有请求最新的数据，那么它就被认为是**不同步**的。持续请求得到最新消息的副本被称为同步的副本，在 Leader 失效时，只有同步副本才有可能被选为新的 Leader。

## 四、处理请求

broker 的大部分工作是处理客户端、分区副本和控制器发送给分区首领的请求。Kafka 提供了一个二进制协议（基于 TCP），指定了请求消息的格式以及 broker 如何对请求作出响应。

broker 会在它所监听的每一个端口上运行一个 Acceptor 线程，这个线程会创建一个连接，并把它交给 Processor 线程去处理。Processor 线程负责从客户端获取请求消息，把它们放进请求队列，然后从响应队列获取响应消息，把它们发送给客户端。

当请求放进请求队列后，IO 线程负责进行处理。

![img](http://dunwu.test.upcdn.net/snap/20200621122854.png)

生产请求和拉取请求都需要发送给分区的 Leader 副本处理。

### 元数据请求

客户端怎么知道哪个是主副本呢？客户端通过使用另一种类型的请求来实现，那就是**元数据请求（metadata request）**。这种请求包含了客户端感兴趣的主题列表。broker 的响应消息指明了这些主题所包含的分区、分区有哪些副本，以及那个副本是 Leader。元数据请求可以发给任意一个 broker，因为所有 broker 都缓存了这些信息。

客户端会把这些信息缓存起来，并直接往目标 broker 上发送生产请求和获取请求。它们需要时不时地通过发送元数据请求来刷新这些信息（刷新的时间间隔通过 `metadata.max.age.ms` 来配置），从而知道元数据是否发生了变化。

![img](http://dunwu.test.upcdn.net/snap/20200621123848.png)

### 生产请求

acks 参数控制多少个副本确认写入成功后生产者才认为消息生产成功。这个参数的取值可以为：

- `acks=0` - 消息发送完毕，生产者认为消息写入成功；
- `acks=1` - 主副本写入成功，生产者认为消息写入成功；
- `acks=all` - 所有同步副本写入成功，生产者才认为消息写入成功。

如果主副本收到生产消息，它会执行一些检查逻辑，包含：

- 发送的用户是否有权限写入主题？
- 请求的 acks 参数取值是否合法（只允许 0，1，all）？
- 如果 acks 设置为 all，是否有足够的 in-sync 副本来安全写入消息？（我们可以配置如果 in-sync 副本低于一定数量，主副本拒绝写入消息）

之后，消息被写入到本地磁盘。一旦消息本地持久化后，如果 acks 被设为 0 或 1 那么会返回结果给客户端，如果 acks 被设为 all 那么会将请求放置在一个称为 purgatory 的缓冲区中等待其他的副本写入完成。

### 消费请求

主副本处理拉取请求和处理生产请求的方式很相似：

1. 请求需要先到达指定的分区主副本上，然后客户端通过查询元数据来确保请求的路由是正确的。
2. 主副本在收到请求时，会先检查请求是否有效。
3. 如果请求的偏移量存在，broker 将按照客户端指定的数量上限从分区里读取消息，再把消息返回给客户端。

**客户端可以指定返回的最大数据量，防止数据量过大造成客户端内存溢出**。同时，**客户端也可以指定返回的最小数据量**，当消息数据量没有达到最小数据量时，请求会一直阻塞直到有足够的数据返回。指定最小的数据量在负载不高的情况下非常有用，通过这种方式**可以减轻网络往返的额外开销**。当然请求也不能永远的阻塞，客户端可以指定最大的阻塞时间，如果到达指定的阻塞时间，即便没有足够的数据也会返回。

Kafka 使用零复制（zero-copy）来提高性能。也就是说，Kafka 将文件（更准确的说，是文件系统缓存）的消息直接传给网络通道，并没有使用中间的 buffer。这避免了内存的字节拷贝和 buffer 维护，极大地提高了性能。

![img](http://dunwu.test.upcdn.net/snap/20200621124516.png)

不是所有主副本的数据都能够被读取。**当数据被所有同步副本写入成功后，它才能被客户端读取**。主副本知道每个消息会被复制到哪个副本上，在消息还没有被写入到所有同步副本之前，是不会发送给消费者的。

因为还没有被足够的副本持久化的消息，被认为是不安全的——如果主副本发生故障，另一个副本成为新的主副本，这些消息就丢失了。如果允许读取这些消息，就可能会破坏数据一致性。

这也意味着，如果 broker 间的消息复制因为某些原因变慢了，那么消息到达消费者的时间也会随之边长。延迟时间可以通过 replica.lag.time.max.ms 来配置，它指定了副本在复制消息时可被允许的最大延迟时间。

![img](http://dunwu.test.upcdn.net/snap/20200621124533.png)

### 其他请求

我们讨论了 Kafka 中最常见的三种请求类型：元信息请求，生产请求和拉取请求。这些请求都是使用的是 Kafka 的自定义二进制协议。集群中 broker 间的通信请求也是使用同样的协议，这些请求是内部使用的，客户端不能发送。比如在选举分区主副本过程中，控制器会发送 LeaderAndIsr 请求给新的主副本和其他跟随副本。

这个协议目前已经支持 20 种请求类型，并且仍然在演进以支持更多的类型。

## 五、物理存储

**Kafka 的基本存储单元是分区**。分区无法在多个 broker 间进行再细分。

### 分区分配

当创建一个新主题时，Kafka 首先需要决定如何分配分区到不同的 broker。分配策略主要的考虑因素如下：

- 尽可能将分区副本均衡分配到集群的 broker 中；
- 对于每个分区，它的所有副本需要在不同的 broker；
- 如果为 broker 制定了机架信息（0.10.0 及更高版本），尽可能地把每个分区的副本分配到不同的机架。

### 文件管理

**Kafka 不会一直保留数据，也不会等待所有的消费者读取了消息才删除消息**。只要数据量达到上限或者数据达到过期时间，Kafka 会删除老的消息数据。

因为在一个大文件中查找和删除消息是非常耗时且容易出错的。所以，Kafka 将每个分区切割成段（segment）。**默认每个段大小不超过 1G，且只包含 7 天的数据**。如果段的消息量达到 1G，那么该段会关闭，同时打开一个新的段进行写入。

**正在写入的段称为活跃段（active segment），活跃段不会被删除**。

对于每个分区的每个段（包括不活跃的段），broker 都会维护文件句柄，因此打开的文件句柄数通常会比较多，这个需要适度调整系统的进程文件句柄参数。

### 文件格式

Kafka 的消息和偏移量保存在文件里。**保存在磁盘上的数据格式和从生产者发送过来或消费者读取的数据格式是一样的。使用相同的数据格式使得 Kafka 可以进行零复制技术给消费者发送消息，同时避免了压缩和解压**。

除了键、值和偏移量外，消息里还包含了消息大小、校验和（检测数据损坏）、魔数（标识消息格式版本）、压缩算法（Snappy、GZip 或者 LZ4）和时间戳（0.10.0 新增）。时间戳可以是生产者发送消息的时间，也可以是消息到达 broker 的时间，这个是可配的。

如果发送者发送压缩的消息，那么批量发送的消息会压缩在一起，以“包装消息”（wrapper message）来发送，如下所示：

![img](http://dunwu.test.upcdn.net/snap/20200621134404.png)

如果生产者使用压缩功能，那么发送更大的批量消息可以得到更好的网络传输效率，并且节省磁盘存储空间。

### 索引

Kafka 允许消费者从任意有效的偏移量位置开始读取消息。Kafka 为每个分区都维护了一个索引，该索引将偏移量映射到片段文件以及偏移量在文件里的位置。

索引也被分成片段，所以在删除消息时，也可以删除相应的索引。Kafka 不维护索引的校验和。如果索引出现损坏，Kafka 会通过重读消息并录制偏移量和位置来重新生成索引。

### 清理

每个日志片段可以分为以下两个部分：

- 干净的部分：这部分消息之前已经被清理过，每个键只存在一个值。
- 污浊的部分：在上一次清理后写入的新消息。

![img](http://dunwu.test.upcdn.net/snap/20200621135557.png)

如果在 Kafka 启动时启用了清理功能（通过 log.cleaner.enabled 配置），每个 broker 会启动一个清理管理器线程和若干个清理线程，每个线程负责一个分区。

清理线程会读取污浊的部分，并在内存里创建一个 map。map 的 key 是消息键的哈希吗，value 是消息的偏移量。对于相同的键，只保留最新的位移。其中 key 的哈希大小为 16 字节，位移大小为 8 个字节。也就是说，一个映射只有 24 字节，假设消息大小为 1KB，那么 1GB 的段有 1 百万条消息，建立这个段的映射只需要 24MB 的内存，映射的内存效率是非常高效的。

在配置 Kafka 时，管理员需要设置这些清理线程可以使用的总内存。如果设置 1GB 的总内存同时有 5 个清理线程，那么每个线程只有 200MB 的内存可用。在清理线程工作时，它不需要把所有脏的段文件都一起在内存中建立上述映射，但需要保证至少能够建立一个段的映射。如果不能同时处理所有脏的段，Kafka 会一次清理最老的几个脏段，然后在下一次再处理其他的脏段。

一旦建立完脏段的键与位移的映射后，清理线程会从最老的干净的段开始处理。如果发现段中的消息的键没有在映射中出现，那么可以知道这个消息是最新的，然后简单的复制到一个新的干净的段中；否则如果消息的键在映射中出现，这条消息需要抛弃，因为对于这个键，已经有新的消息写入。处理完会将产生的新段替代原始段，并处理下一个段。

对于一个段，清理前后的效果如下：

![img](http://dunwu.test.upcdn.net/snap/20200621140117.png)

### 删除事件

对于只保留最新消息的清理策略来说，Kafka 还支持删除相应键的消息操作（而不仅仅是保留最新的消息内容）。这是通过生产者发送一条特殊的消息来实现的，该消息包含一个键以及一个 null 的消息内容。当清理线程发现这条消息时，它首先仍然进行一个正常的清理并且保留这个包含 null 的特殊消息一段时间，在这段时间内消费者消费者可以获取到这条消息并且知道消息内容已经被删除。过了这段时间，清理线程会删除这条消息，这个键会从分区中消失。这段时间是必须的，因为它可以使得消费者有一定的时间余地来收到这条消息。

## 参考资料

- **官方**
  - [Kakfa 官网](http://kafka.apache.org/)
  - [Kakfa Github](https://github.com/apache/kafka)
  - [Kakfa 官方文档](https://kafka.apache.org/documentation/)
- **书籍**
  - [《Kafka 权威指南》](https://item.jd.com/12270295.html)
- **教程**
  - [Kafka 中文文档](https://github.com/apachecn/kafka-doc-zh)
- **文章**
  - [Kafka(03) Kafka 介绍](http://www.heartthinkdo.com/?p=2006#233)
  - [Kafka 剖析（一）：Kafka 背景及架构介绍](http://www.infoq.com/cn/articles/kafka-analysis-part-1)
  - [Thorough Introduction to Apache Kafka](https://hackernoon.com/thorough-introduction-to-apache-kafka-6fbf2989bbc1)
  - [Kafak(04) Kafka 生产者事务和幂等](http://www.heartthinkdo.com/?p=2040#43)
  - [https://cwiki.apache.org/confluence/display/KAFKA/Kafka+data+structures+in+Zookeeper](https://cwiki.apache.org/confluence/display/KAFKA/Kafka+data+structures+in+Zookeeper)
